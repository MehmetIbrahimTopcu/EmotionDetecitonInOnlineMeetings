{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# For truncated image loading\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Device selection\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data loading\n",
    "labels = []\n",
    "file_names = []\n",
    "base_path = Path(r\"C:\\Users\\mibra\\Downloads\\archive (4)\\fer_ckplus_kdef\")\n",
    "\n",
    "for file in sorted(base_path.rglob('*.*')):\n",
    "    label = file.parent.name\n",
    "    if label not in [\"contempt\", \"fear\", \"disgust\"]:  # Cleaned control\n",
    "        labels.append(label)\n",
    "        file_names.append(str(file))\n",
    "\n",
    "print(f\"Total files: {len(file_names)}, Label count: {len(labels)}\")\n",
    "\n",
    "df = pd.DataFrame({\"image\": file_names, \"label\": labels})\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(\"Class distribution:\", Counter(labels))\n",
    "\n",
    "y = df[\"label\"].tolist()\n",
    "x = df[\"image\"].tolist()\n",
    "\n",
    "def balance_with_augmentation_optimized(X_paths, y_labels, img_size=(224, 224)):\n",
    "    \"\"\"Memory-efficient augmentation and balancing for grayscale images\"\"\"\n",
    "\n",
    "    # Albumentations transform - optimized for grayscale\n",
    "    aug_transform = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=20, p=0.6),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Affine(\n",
    "            translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "            scale={\"x\": (0.85, 1.15), \"y\": (0.85, 1.15)},\n",
    "            rotate=(-15, 15),\n",
    "            p=0.5\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    # Calculate class distribution\n",
    "    class_counts = Counter(y_labels)\n",
    "    mean_count = int(np.mean(list(class_counts.values())))\n",
    "\n",
    "    print(f\"Original class distribution: {class_counts}\")\n",
    "    print(f\"Average class count: {mean_count}\")\n",
    "    print(f\"Target class count (mean-based): {mean_count}\")\n",
    "\n",
    "    X_balanced = []\n",
    "    y_balanced = []\n",
    "\n",
    "    print(\"Starting augmentation and balancing process...\")\n",
    "\n",
    "    for idx, path in enumerate(tqdm(X_paths)):\n",
    "        # Read as grayscale\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Warning: {path} file could not be read, skipping...\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.resize(img, img_size)\n",
    "\n",
    "        # Image enhancement\n",
    "        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "        img = cv2.filter2D(img, -1, kernel)\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img = clahe.apply(img)\n",
    "\n",
    "        label = y_labels[idx]\n",
    "\n",
    "        # Add original image\n",
    "        X_balanced.append(img.flatten())\n",
    "        y_balanced.append(label)\n",
    "\n",
    "        # Augmentation only for minority classes (those below mean)\n",
    "        current_count = class_counts[label]\n",
    "        if current_count < mean_count:\n",
    "            needed_augmentations = min((mean_count - current_count) // current_count, 3)\n",
    "\n",
    "            for _ in range(needed_augmentations):\n",
    "                # Convert to 3 channels\n",
    "                img_3ch = np.stack([img, img, img], axis=-1)\n",
    "\n",
    "                try:\n",
    "                    augmented = aug_transform(image=img_3ch)[\"image\"]\n",
    "                    augmented_gray = cv2.cvtColor(augmented, cv2.COLOR_RGB2GRAY)\n",
    "                    augmented_gray = cv2.resize(augmented_gray, img_size)\n",
    "\n",
    "                    # Enhancement\n",
    "                    augmented_gray = cv2.filter2D(augmented_gray, -1, kernel)\n",
    "                    augmented_gray = clahe.apply(augmented_gray)\n",
    "\n",
    "                    X_balanced.append(augmented_gray.flatten())\n",
    "                    y_balanced.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Augmentation error: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # Convert to NumPy array\n",
    "    X_array = np.array(X_balanced, dtype=np.uint8)\n",
    "    y_array = np.array(y_balanced)\n",
    "\n",
    "    print(f\"Total samples after augmentation: {len(X_array)}\")\n",
    "\n",
    "    # Final balancing with RandomOverSampler\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_array, y_array)\n",
    "\n",
    "    # Reshape back to image format\n",
    "    X_final = X_resampled.reshape(-1, img_size[0], img_size[1])\n",
    "\n",
    "    print(f\"Final balanced dataset: {len(X_final)} samples\")\n",
    "    print(f\"Final class distribution: {Counter(y_resampled)}\")\n",
    "\n",
    "    return X_final, y_resampled\n",
    "\n",
    "# Data preparation\n",
    "X_final, y_final = balance_with_augmentation_optimized(x, y)\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_final)\n",
    "\n",
    "# Save label encoder\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(f\"Label classes: {le.classes_}\")\n",
    "print(f\"Total samples after balancing: {len(y_encoded)}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.X[idx].astype(np.uint8)\n",
    "        label = self.y[idx]\n",
    "\n",
    "        try:\n",
    "            img_pil = Image.fromarray(img, mode='L')\n",
    "\n",
    "            if self.transform:\n",
    "                img_tensor = self.transform(img_pil)\n",
    "            else:\n",
    "                img_tensor = transforms.ToTensor()(img_pil)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Transform error: {e}\")\n",
    "            img_tensor = torch.zeros((3, 224, 224))\n",
    "\n",
    "        return img_tensor, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = EmotionDataset(X_train, y_train, transform=transform_train)\n",
    "test_dataset = EmotionDataset(X_test, y_test, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Model creation\n",
    "model = models.mobilenet_v3_small(pretrained=True)\n",
    "\n",
    "# Adjust classifier for number of classes - THIS IS IMPORTANT!\n",
    "num_classes = len(le.classes_)\n",
    "model.classifier[3] = nn.Linear(in_features=1024, out_features=num_classes)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model output classes: {num_classes}\")\n",
    "print(f\"Model classes: {le.classes_}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.7)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    \"\"\"Advanced training function\"\"\"\n",
    "    best_acc = 0.0\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Training start time\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Training start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct_test / total_test\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            # Timestamped model save\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "            model_path = f'best_emotion_model_{timestamp}.pth'\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"New best accuracy: {best_acc:.2f}% - Model saved: {model_path}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Learning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    training_duration = end_time - start_time\n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Training duration: {training_duration}\")\n",
    "    print(f\"Best test accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "    return train_losses, test_accuracies, best_acc\n",
    "\n",
    "# Model training\n",
    "print(\"Starting model training...\")\n",
    "train_losses, test_accuracies, best_accuracy = train_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=10\n",
    ")\n",
    "\n",
    "# Save final model\n",
    "final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "final_model_path = f'final_emotion_model_{final_timestamp}.pth'\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save metadata\n",
    "model_info = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': le.classes_.tolist(),\n",
    "    'best_accuracy': best_accuracy,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'model_architecture': 'MobileNetV3-Small',\n",
    "    'input_size': (224, 224),\n",
    "    'final_train_loss': train_losses[-1] if train_losses else None,\n",
    "    'final_test_accuracy': test_accuracies[-1] if test_accuracies else None\n",
    "}\n",
    "\n",
    "metadata_path = f'model_info_{final_timestamp}.pkl'\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "print(f\"Model metadata saved: {metadata_path}\")\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Per-class accuracy\n",
    "        for i in range(labels.size(0)):\n",
    "            label_idx = labels[i].item()\n",
    "            pred_idx = predicted[i].item()\n",
    "\n",
    "            class_total[label_idx] += 1\n",
    "            if label_idx == pred_idx:\n",
    "                class_correct[label_idx] += 1\n",
    "\n",
    "final_accuracy = 100 * correct / total\n",
    "print(f'Final Test Accuracy: {final_accuracy:.2f}%')\n",
    "print(f'Total test samples: {total}')\n",
    "\n",
    "# Class-wise accuracy\n",
    "print(f\"\\nClass-wise Accuracies:\")\n",
    "print(\"-\" * 50)\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    if class_total[i] > 0:\n",
    "        class_acc = 100 * class_correct[i] / class_total[i]\n",
    "        print(f'{class_name:>12}: {class_acc:>6.2f}% ({class_correct[i]:>4}/{class_total[i]:>4})')\n",
    "    else:\n",
    "        print(f'{class_name:>12}: No samples in test set')\n",
    "\n",
    "# Detailed classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Collect all predictions\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(\"-\" * 60)\n",
    "report = classification_report(all_labels, all_predictions,\n",
    "                               target_names=le.classes_, digits=3)\n",
    "print(report)\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title(f'Confusion Matrix - Accuracy: {final_accuracy:.2f}%')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix\n",
    "cm_path = f'confusion_matrix_{final_timestamp}.png'\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion matrix saved: {cm_path}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best model saved as: best_emotion_model_{final_timestamp}.pth\")\n",
    "print(f\"Final model saved as: {final_model_path}\")\n",
    "print(f\"Metadata saved as: {metadata_path}\")\n",
    "print(f\"Confusion matrix: {cm_path}\")\n",
    "print(f\"Best accuracy achieved: {best_accuracy:.2f}%\")\n",
    "print(f\"Final test accuracy: {final_accuracy:.2f}%\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {', '.join(le.classes_)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prediction function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_emotion_from_path(image_path, model, le, transform, device, show_image=True, show_probabilities=True):\n",
    "    \"\"\"\n",
    "    Performs emotion prediction using a single image path\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to image file\n",
    "        model: Trained PyTorch model\n",
    "        le: LabelEncoder object\n",
    "        transform: Preprocessing transforms\n",
    "        device: PyTorch device (cuda/cpu)\n",
    "        show_image (bool): Show/hide image\n",
    "        show_probabilities (bool): Show probabilities for all classes\n",
    "\n",
    "    Returns:\n",
    "        dict: Prediction results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load image and check\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Error: {image_path} file not found!\")\n",
    "            return None\n",
    "\n",
    "        # Open image with PIL\n",
    "        image = Image.open(image_path)\n",
    "        original_image = image.copy()  # Save original image for display\n",
    "\n",
    "        # Show image (optional)\n",
    "        if show_image:\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.imshow(image, cmap='gray' if image.mode == 'L' else None)\n",
    "            plt.title(f\"Input Image: {os.path.basename(image_path)}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Transform image\n",
    "        if transform:\n",
    "            input_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "        else:\n",
    "            # Fallback transform\n",
    "            input_tensor = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.Grayscale(num_output_channels=3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])(image).unsqueeze(0)\n",
    "\n",
    "        # Move to GPU\n",
    "        input_tensor = input_tensor.to(device)\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # Highest probability and class\n",
    "        max_prob, predicted_class = torch.max(probabilities, 1)\n",
    "        predicted_emotion = le.classes_[predicted_class.item()]\n",
    "        confidence = max_prob.item()\n",
    "\n",
    "        # Probabilities for all classes\n",
    "        all_probabilities = probabilities[0].cpu().numpy()\n",
    "        emotion_probabilities = {}\n",
    "\n",
    "        for i, emotion in enumerate(le.classes_):\n",
    "            emotion_probabilities[emotion] = all_probabilities[i]\n",
    "\n",
    "        # Sort results (highest to lowest)\n",
    "        sorted_emotions = sorted(emotion_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EMOTION PREDICTION RESULTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Image: {os.path.basename(image_path)}\")\n",
    "        print(f\"Predicted Emotion: {predicted_emotion.upper()}\")\n",
    "        print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "        print(f\"Device: {device}\")\n",
    "\n",
    "        if show_probabilities:\n",
    "            print(f\"\\nAll Emotion Probabilities:\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            for emotion, prob in sorted_emotions:\n",
    "                bar = \"â–ˆ\" * int(prob * 20)  # Simple progress bar\n",
    "                print(f\"{emotion:>12}: {prob*100:>6.2f}% {bar}\")\n",
    "\n",
    "        # Return dictionary\n",
    "        result = {\n",
    "            'predicted_emotion': predicted_emotion,\n",
    "            'confidence': confidence,\n",
    "            'all_probabilities': emotion_probabilities,\n",
    "            'sorted_probabilities': sorted_emotions,\n",
    "            'image_path': image_path\n",
    "        }\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Transform for prediction\n",
    "transform_predict = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Example usage:\n",
    "# image_path = r\"path/to/your/image.jpg\"\n",
    "# result = predict_emotion_from_path(image_path, model, le, transform_predict, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
